{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4502624,"sourceType":"datasetVersion","datasetId":2632286}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Is All you Need\n\n### The original transformer\nThis notebook is an attempt to implement the classic paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) (Vasmanit et al, 2017) with pytorch. *Attention* was previously used in RNN encoder-decoder models which have the disadvantage of having to loop over the entire sequence. The authors dispens with recurrence and show how *self-attention*, can be used as the *heart* of the model.\n\nThe paper written in 2017 had a significant impact on NLP and deep learning and paved the way for later breakthroughs such as BERT or GPT-3. The model of 2017 is sometimes refered to as the *original transformer*. \n\n<img src=\"https://pbs.twimg.com/media/DCYHlQCUMAAsLhG?format=jpg&name=small\" style=\"width:512px;height:384px;\" alt=\"Dwight's Attention\">\n\n## About this Notebook üë®‚Äçüíª\nThe transformer is trained to translate sentences from French to English . I use a rather small dataset with English-French sentence pairs (e.g. \"I don't know\", \"Je ne sais pas\").\n\nThe goal was to make a simple end to end example and to undestand the attention-mechanism as well as the model architechture of the first transformer. For a long time the model did not learn as expected. Despite the loss getting smaller every epoch and \"semi-sain\"inference, the model would never converge üßê . I could finally fix it by borrowing the code for the sublayers from [the annotated transformer](http://nlp.seas.harvard.edu/annotated-transformer/#embeddings-and-softmax). Another good source is Peter Bloem's [blog post](https://peterbloem.nl/blog/transformers). \n\nSince the original paper is really well written, I mostly quote from it directly to explain what's happening. (All quotes are from the paper *Attention is all you need* )","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import LambdaLR\nimport math \nimport numpy as np\nimport unicodedata\nimport string\nimport re\nimport random\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device :\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2026-01-11T15:04:11.638268Z","iopub.execute_input":"2026-01-11T15:04:11.638615Z","iopub.status.idle":"2026-01-11T15:04:14.100536Z","shell.execute_reply.started":"2026-01-11T15:04:11.638546Z","shell.execute_reply":"2026-01-11T15:04:14.099560Z"},"trusted":true},"outputs":[{"name":"stdout","text":"device : cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Text Preprocessing ü§ñ\n\nNLP involves a lot of data wrangling. To stick to the simple but insightful example I won't use the usual convenience functions (for example by hugginface) to create the text-pipeline. \nTokenizer, Vocabulary, Batch-Makers are coded with good ol' python. \n\n*The code for the tokenizer and vocab builder are from [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) excellent torch tutorial!*","metadata":{}},{"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\nPAD_token = 2\nMAX_LENGTH = 50\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"}\n        self.n_words = 3  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\ndef readLangs(lang1, lang2, reverse=False):\n    lines = open('../input/english-french/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n        read().strip().split('\\n')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH \n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:14.102332Z","iopub.execute_input":"2026-01-11T15:04:14.102739Z","iopub.status.idle":"2026-01-11T15:04:14.114478Z","shell.execute_reply.started":"2026-01-11T15:04:14.102713Z","shell.execute_reply":"2026-01-11T15:04:14.113454Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepareData('eng', 'fra', True)\nprint(random.choice(pairs))","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:14.115647Z","iopub.execute_input":"2026-01-11T15:04:14.115939Z","iopub.status.idle":"2026-01-11T15:04:19.649060Z","shell.execute_reply.started":"2026-01-11T15:04:14.115903Z","shell.execute_reply":"2026-01-11T15:04:19.648144Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Read 135842 sentence pairs\nTrimmed to 135837 sentence pairs\nCounting words...\nCounted words:\nfra 21326\neng 13039\n['voudrais tu te presenter ?', 'would you introduce yourself ?']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### From sentence to indexes and batches\n\nBellow are the functions that transform sentences into lists of indexes and then creates batches. ","metadata":{}},{"cell_type":"code","source":"def indexes_from_sentence(lang, sentence):\n    idxs = [lang.word2index[word] for word in sentence.split(' ')]\n    idxs.append(EOS_token)\n    idxs.insert(SOS_token,0)\n    return idxs\n    \ndef batch_from_pairs(pairs):\n    batch_inp = [indexes_from_sentence(input_lang, p[0]) for p in pairs]\n    longest_seq = max([len(seq) for seq in batch_inp])\n    batch_inp = [seq+[PAD_token]*(longest_seq-len(seq)) for seq in batch_inp]\n    input_tensor = torch.tensor(batch_inp, dtype=torch.long, device=device)\n    \n    batch_trg = [indexes_from_sentence(output_lang, p[1]) for p in pairs]\n    longest_seq = max([len(seq) for seq in batch_trg])\n    batch_trg = [seq+[PAD_token]*(longest_seq-len(seq)) for seq in batch_trg]\n    target_tensor = torch.tensor(batch_trg, dtype=torch.long, device=device)    \n    \n    return input_tensor,target_tensor","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.650062Z","iopub.execute_input":"2026-01-11T15:04:19.650347Z","iopub.status.idle":"2026-01-11T15:04:19.659933Z","shell.execute_reply.started":"2026-01-11T15:04:19.650322Z","shell.execute_reply":"2026-01-11T15:04:19.658920Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Implementing the Transformer \n\nHere is a high level overview. It's actually just that.<br>\nSo, once you understand all the parts you can come back and marvel at the explanatory power of this illustration.üí™üèãÔ∏è‚Äç","metadata":{}},{"cell_type":"markdown","source":"![Model Architecture](https://www.researchgate.net/profile/Dennis-Gannon-2/publication/339390384/figure/fig1/AS:860759328321536@1582232424168/The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.jpg)\n\n### Some further notes to the illustration\n\n* It is encoder-decoder architecture. \n* The encoder creates features of the input sequence.\n* The decoder uses the output of the encoder to decode the target sequence.\n* The decoder uses the shifted targets during training. During prediction the outputs are fed back as the new inputs.\n* Both encoder are using stacked self-attention and point-wise, fully connected layers.\n\nLet's go through it step by step.","metadata":{}},{"cell_type":"markdown","source":"## Attention!\n\nSo here is the magic sauce. ü™Ñ\nMy intution of attention is the following. Usually when we feed our input vectors into to a linear layer the vectors do not *see each other*. There is no interaction. Well, with attention, they DO *see each other*. Look at the formula bellow. The dot product of the keys and values is some sort of interbreeding. ‚ù§Ô∏è (Key and values are linear projections of the same vectors.) Through the dot product with the values, the most intersting interactions are then selected. So, we get context-enriched, interaction heavy feature vectors as ouput. So that no illegal interactions happen (appart from the incest), a mask is applied to prevent interactions with \"the future\". (Words can only flirt with words that come before.)\n\nIf you perform (scaled-dot-product) attention several times you get *multlihead attention*. Let's see what the experts say about that.","metadata":{}},{"cell_type":"markdown","source":"###  Scaled dot-product attention\nThe authors wrote:\n\n> We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by ‚àö\ndk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\n\n![Attention function](https://miro.medium.com/max/720/1*P9sV1xXM10t943bXy_G9yg.png)\n\n### Multihead attention\n\n> Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the final values ... Multi-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n","metadata":{}},{"cell_type":"code","source":"\ndef attention(q,k,v,dropout,mask=None):\n    b,h,l,dk = q.size()\n    x = torch.matmul(q,k.transpose(-2,-1)) / dk**0.5\n    \n    if mask is not None:\n        x= x.masked_fill(mask==0,-1e9)\n    x = x.softmax(dim=-1)\n    x = dropout(x)    \n    x = torch.matmul(x,v)\n    return x\n\nclass MultiHeaderAttention(nn.Module):\n    def __init__(self,d_model,dropout,n_heads=8, dk=64,dv=64):\n        super(MultiHeaderAttention,self).__init__()\n        self.dims = dk,dv,n_heads\n        self.q = nn.Linear(d_model, dk*n_heads)\n        self.k = nn.Linear(d_model, dk*n_heads)\n        self.v = nn.Linear(d_model, dv*n_heads)\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.out = nn.Linear(dv*n_heads, d_model)\n            \n    def forward(self,k,v,q,mask=None):\n        b,len_k,len_q,len_v = k.size(0), k.size(1), q.size(1), v.size(1)\n        dk,dv,h =self.dims\n        q = self.q(q).view(b,len_q,h,dk).transpose(1,2)\n        k = self.k(k).view(b,len_k,h,dk).transpose(1,2)\n        v = self.v(v).view(b,len_v,h,dk).transpose(1,2)\n        if mask is not None:\n            mask = mask.unsqueeze(1) #put header dim for broadcasting\n            \n        x = attention(q,k,v, self.dropout, mask)\n        x = x.transpose(1,2).contiguous().view(b,len_q,h*dk) # swap headers and seq_len\n        return self.out(x)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.661247Z","iopub.execute_input":"2026-01-11T15:04:19.661767Z","iopub.status.idle":"2026-01-11T15:04:19.680002Z","shell.execute_reply.started":"2026-01-11T15:04:19.661731Z","shell.execute_reply":"2026-01-11T15:04:19.679053Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Positional encoding\n\nThe model needs to know the order of the words in a sentence. (Or more precisely, the order of the embeddings)\nIn the paper they do this by adding a cyclical signal to the word embeddings.\n\nThe authors write:\n> Since our model contains no recurrence and no convolution, in order for the model to make use of the\n> order of the sequence, we must inject some information about the relative or absolute position of the\"\n\nand\n\n> To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000* 2i/dmodel) PE(pos,2i+1) = cos(pos/10000 * 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄ to 10000 ¬∑ 2œÄ. ","metadata":{}},{"cell_type":"code","source":"class PositionEncoding(nn.Module):\n    def __init__(self,max_len,d_model):\n        super(PositionEncoding,self).__init__()\n        self.max_len = max_len+5\n        self.register_buffer('pos_table', self.tensor_pos_encoding(self.max_len, d_model))\n\n    def pos_encoding(self,pos, k):\n        \"\"\"taking an vocab index and generating a a geometric progression with k dimensions \"\"\"\n        f = lambda i,k: pos / 10000**(2 * (i // 2) / k)\n        return [math.sin(f(i,k)) if i%2==0 else math.cos(f(i,k)) for i in range(0,k)]\n\n    def tensor_pos_encoding(self,max_len,dim):\n        return torch.tensor([self.pos_encoding(i,dim) for i in range(max_len)],device=device).view( max_len,dim)\n\n    def forward(self,x):\n        return x+ self.pos_table[:x.size(1),:].detach().clone().unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.681043Z","iopub.execute_input":"2026-01-11T15:04:19.681314Z","iopub.status.idle":"2026-01-11T15:04:19.692398Z","shell.execute_reply.started":"2026-01-11T15:04:19.681275Z","shell.execute_reply":"2026-01-11T15:04:19.691515Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Subylayer connection\n\nSometimes it's good to learn from the past. That's basically what residual connections do. \n\n> We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. \n\nConcering dropout:\n> We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.","metadata":{}},{"cell_type":"code","source":"# code from \"the annotated transfomer\"\nclass SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = nn.LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.695642Z","iopub.execute_input":"2026-01-11T15:04:19.696265Z","iopub.status.idle":"2026-01-11T15:04:19.705472Z","shell.execute_reply.started":"2026-01-11T15:04:19.696225Z","shell.execute_reply":"2026-01-11T15:04:19.704542Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Encoder\n\nThe encoder will take our French sentences an *encode* it. Imagine that you are engineering features for your favorite gradient booster model. The output of the encoder is basically the features of our French sentences, ready to be fed into the decoder.\n\nThe authors description of the encoder:\n> The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\n\nThe application of attention in the encoder is described as followed:\n> The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, n_input_vocab, d_model,n_hidden,n_layers,dropout):\n        super().__init__()\n        self.d_model = d_model\n        \n        self.embedding = nn.Embedding(n_input_vocab,d_model,padding_idx=PAD_token)\n        self.dropout = nn.Dropout(p=dropout)\n        self.normal = nn.LayerNorm(d_model, eps=1e-6)   \n        self.encoder_layers = nn.ModuleList(\n            [EncoderLayer(d_model, n_hidden,dropout) for i in range(n_layers)]\n            )\n\n        self.pos_enc = PositionEncoding(MAX_LENGTH,d_model)\n\n        \n    def forward(self,x,mask):\n        x = self.embedding(x) * self.d_model**0.5\n        x = self.pos_enc(x)\n        \n        #stack of N = 6 identical layers\n        for layer in self.encoder_layers:\n            x = layer(x,mask)\n        return self.normal(x)\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, n_hidden,dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = attn = MultiHeaderAttention(d_model,dropout)\n        self.feed_forward = nn.Sequential(\n                        nn.Linear(d_model,n_hidden), \n                        nn.ReLU(),\n                        nn.Dropout(p=dropout),\n                        nn.Linear(n_hidden,d_model) \n        )\n        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for i in range(2)])\n\n    def forward(self, x, mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.706624Z","iopub.execute_input":"2026-01-11T15:04:19.706938Z","iopub.status.idle":"2026-01-11T15:04:19.717466Z","shell.execute_reply.started":"2026-01-11T15:04:19.706905Z","shell.execute_reply":"2026-01-11T15:04:19.716702Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Decoder\n\nThe decoder will take the encoder output (features of the French sentences) and the target sequence (here the English sentences) and decode it. The decoder output is then fed into a linear layer whose job it is to predict the next word of each vector of the sequence. (See how the labels are made during training to understand why.) \n\n> The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n\n\nThe first application of the attention in the decoder (sublayer1) is described as follows:\n> Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n\nThe second application of the attention in the decoder (sublayer2) is described as follows:\n> In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. ","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self,n_target_vocab,d_model, n_hidden,n_layers,dropout):\n        super().__init__()\n        self.d_model = d_model\n        \n        self.embedding = nn.Embedding(n_target_vocab,d_model,padding_idx=PAD_token)\n        self.dropout = nn.Dropout(p=dropout)\n        self.normal = nn.LayerNorm(d_model, eps=1e-6)\n        self.decoder_layers = nn.ModuleList(\n            [DecoderLayer(d_model, n_hidden,dropout) for i in range(n_layers)]\n            )\n            \n        self.pos_enc = PositionEncoding(MAX_LENGTH,d_model)\n        \n    def forward(self,x,encoder_outputs,self_attn_mask, enc_dec_mask):\n        x = self.embedding(x)*self.d_model**0.5\n        x = self.pos_enc(x)\n        \n        #stack of N = 6 identical layers\n        for layer in self.decoder_layers:\n            x = layer(x,encoder_outputs,self_attn_mask, enc_dec_mask)\n        \n        return self.normal(x)\n    \nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_hidden,dropout):\n        super(DecoderLayer, self).__init__()\n\n        self.self_attn = MultiHeaderAttention(d_model,dropout)\n        self.src_attn = MultiHeaderAttention(d_model,dropout)\n        self.feed_forward = nn.Sequential(\n                        nn.Linear(d_model,n_hidden), \n                        nn.ReLU(),\n                        nn.Dropout(p=dropout),\n                        nn.Linear(n_hidden,d_model) \n        )\n        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for i in range(3)])\n\n    def forward(self, x, memory, tgt_mask,src_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(m, m, x,src_mask))\n        return self.sublayer[2](x, self.feed_forward)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.718433Z","iopub.execute_input":"2026-01-11T15:04:19.718680Z","iopub.status.idle":"2026-01-11T15:04:19.729685Z","shell.execute_reply.started":"2026-01-11T15:04:19.718649Z","shell.execute_reply":"2026-01-11T15:04:19.728989Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## The Transfomer \n\nNow we have all the pieces for the transformer. Here, the encoder and decoder are instanciated, the masks that prevent looking at irrelevant (pad tokens) or illegal (looking ahead) content are generated. A linear layer outputs the final guesses of the word distributions for the whole sequence. Note that the Softmax functions are not implemented. This is because the Crossentropy Loss expects the raw output. ","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,d_model, n_input_vocab,n_target_vocab, n_hidden,n_layers,dropout):\n        super().__init__()\n\n        self.encoder = Encoder(n_input_vocab=n_input_vocab, d_model=d_model,n_hidden=n_hidden,n_layers=n_layers,dropout=dropout)\n        self.decoder = Decoder(n_target_vocab=n_target_vocab,d_model=d_model,n_hidden=n_hidden,n_layers=n_layers,dropout=dropout)\n        self.out = nn.Linear(d_model,n_target_vocab)  \n        \n    def get_target_mask(self, target_seq):\n        b_sz, len_s = target_seq.size()\n        return torch.tril(torch.ones(len_s,len_s,device=device)).bool().expand(1,len_s,len_s)\n\n    def get_pad_mask(self,seq):\n        return (seq != PAD_token).unsqueeze(-2)   \n    \n    def forward(self,input_seq, target_seq):\n        trg_mask = self.get_pad_mask(target_seq)\n        trg_mask = trg_mask & self.get_target_mask(target_seq).type_as(trg_mask.data)\n        inp_mask = self.get_pad_mask(input_seq)\n\n        encoder_out = self.encoder(input_seq,inp_mask)\n        decoder_out = self.decoder(target_seq,encoder_out,trg_mask,inp_mask)\n        \n        out = self.out(decoder_out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.730762Z","iopub.execute_input":"2026-01-11T15:04:19.731267Z","iopub.status.idle":"2026-01-11T15:04:19.740861Z","shell.execute_reply.started":"2026-01-11T15:04:19.731242Z","shell.execute_reply":"2026-01-11T15:04:19.740197Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training & Random Tests üí™","metadata":{}},{"cell_type":"markdown","source":"It's training time. First we shuffle the dataset and split it into training and test partitions. Then we instatiate the model and set up the optimizer and scheduler. Finally we run the training loop. We get a hot cup of coffee, lean back and look mesmerized at the ever decreasing loss. ‚òïÔ∏è","metadata":{}},{"cell_type":"code","source":"#splitting the data into train and test datasets\n\nnp.random.shuffle(pairs)\ntrain = pairs[:-1000]\ntest = pairs[-1000:]\n\n#the model with the parameter-values of the paper\ntransformer1 = Transformer(d_model=512, \n                            n_input_vocab=input_lang.n_words, \n                            n_target_vocab= output_lang.n_words,\n                            n_hidden = 2048,\n                            n_layers=6, \n                            dropout=0.1).to(device)\n\nfor p in transformer1.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:19.741964Z","iopub.execute_input":"2026-01-11T15:04:19.742205Z","iopub.status.idle":"2026-01-11T15:04:23.806645Z","shell.execute_reply.started":"2026-01-11T15:04:19.742182Z","shell.execute_reply":"2026-01-11T15:04:23.805657Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Optimizer \n\n> We used the Adam optimizer [20] with Œ≤1 = 0.9, Œ≤2 = 0.98 and \u000f = 10‚àí9\n. We varied the learning\nrate over the course of training, according to the formula:<br>\n`lrate = d_model**‚àí0.5 model ¬∑ min(step_num‚àí0.5, step_num ¬∑ warmup_steps‚àí1.5)` (3)<br>\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.","metadata":{}},{"cell_type":"code","source":"#the optimizer \nlr= 1\nopt1 = optim.Adam(transformer1.parameters(),lr=lr, betas=(0.9, 0.98), eps=1e-09)\n\ndef lr_rate(step_num, d_model, factor, warmup_steps):\n    step_num =max(1,step_num)\n    return factor * (\n        d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n    )\n\nlr_scheduler = LambdaLR(\n    optimizer=opt1,\n    lr_lambda=lambda step_num: lr_rate(\n        step_num, 512, factor=1, warmup_steps=4000\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:23.807827Z","iopub.execute_input":"2026-01-11T15:04:23.808195Z","iopub.status.idle":"2026-01-11T15:04:23.815395Z","shell.execute_reply.started":"2026-01-11T15:04:23.808162Z","shell.execute_reply":"2026-01-11T15:04:23.814476Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Training and evaluation functions\n\nLet's define a function to test the model with random samples from the test set. This will make the learning process more intuitive. Ultimately, we want to see the model translate something it has not seen before and the better the quality of this, the more we will be impressed. üòÄ","metadata":{}},{"cell_type":"code","source":"def pred(input_seq, model):\n    outputs = [SOS_token]\n    \n    loss = 0\n    for i in range(MAX_LENGTH):\n        target_seq = torch.tensor([outputs],device=device)\n        output = model(input_seq,target_seq)\n        probs = F.softmax(output,dim=2)\n        word_pred = torch.argmax(probs[:,-1,:],dim=1)\n\n        outputs.append(word_pred.item())\n\n        if word_pred.item()== EOS_token:\n            break\n    \n    return outputs[1:]\n\ndef random_model_testing(n_examples,model):\n    batch_sz=1\n    test_samples = [random.choice(test) for i in range(n_samples)]\n    print(\"Random Tests\")\n    print(\"*\"*30)\n    for i in range(0,len(test_samples[:n_examples]),batch_sz):\n        input_tensor, output_tensor = batch_from_pairs(test_samples[i:i+batch_sz])\n        out = pred(input_tensor, model)\n        print(\"Pred: \", \" \".join([output_lang.index2word[i] for i in out]), \"True: \",test_samples[i][1])\n    print(\"*\"*30)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:23.816264Z","iopub.execute_input":"2026-01-11T15:04:23.816526Z","iopub.status.idle":"2026-01-11T15:04:23.828260Z","shell.execute_reply.started":"2026-01-11T15:04:23.816504Z","shell.execute_reply":"2026-01-11T15:04:23.827355Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"The \"train_batch\" function takes in a batch, runs that batch through the model, calculates the loss and runs backpropagation. Also, the learning rate is adjusted by calling `scheduler.step()`.\\\nLabel smoothing is applied according to the paper:\n>During training, we employed label smoothing of value \u000fls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.","metadata":{}},{"cell_type":"code","source":"def train_batch(input_seq, target_seq, model, optimizer,scheduler):\n    target, truth = target_seq[:,:-1], target_seq[:,1:]\n    pred = model(input_seq,target)\n    \n    loss = F.cross_entropy(pred.view(-1,output_lang.n_words), truth.reshape(-1),reduction='sum',label_smoothing=0.1)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    scheduler.step()\n\n    return loss.item()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:23.829406Z","iopub.execute_input":"2026-01-11T15:04:23.829681Z","iopub.status.idle":"2026-01-11T15:04:23.838867Z","shell.execute_reply.started":"2026-01-11T15:04:23.829652Z","shell.execute_reply":"2026-01-11T15:04:23.838146Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Let's train our transfomer!\n\nBellow is a simple training loop. Each epoch we feed the input and target batches to the model, calculate the loss and backpropagate. To make training a bit more interesting, we draw a small random a sample for each epoch instead of iterating over all ~135k pairs. It is easier to track the progress like that. \n\nTo make the learning process more intuitive, the function \"random model testing\" is called every 25 epochs, where the model translates 25 sentences from French to English. We see that the model gets \"saner\" as we go. For example, after 25 epochs the model outputs **\"go and do you get the hair !\"** instead of **\"get a haircut\"**. So we put our model back in the oven and wait for it to mature...\n\nAfter 50 epochs, instead of **\"you re the most important person in my life \"** the model output is **\"you are the one in my life\"**. Alright, why not? A philosopher... ü§î","metadata":{}},{"cell_type":"code","source":"n_samples=21000\nepochs =200\nbatch_sz=32\n\nfor e in range(epochs):\n    loss = 0\n    train_samples = [random.choice(train) for i in range(n_samples)]\n    for i in range(0,len(train_samples),batch_sz):\n        input_tensor, output_tensor = batch_from_pairs(train_samples[i:i+batch_sz])\n        loss += train_batch(input_tensor, output_tensor, transformer1, opt1,lr_scheduler)\n    print(f\"Epoch {e}/{epochs} | loss: {round(loss/n_samples,2)} | learning rate: {round(lr_scheduler.get_last_lr()[0],6)}\")\n    #random testing\n    if e%25==0:\n        random_model_testing(10,transformer1)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T15:04:23.840001Z","iopub.execute_input":"2026-01-11T15:04:23.840511Z","iopub.status.idle":"2026-01-11T18:26:27.363151Z","shell.execute_reply.started":"2026-01-11T15:04:23.840478Z","shell.execute_reply":"2026-01-11T18:26:27.362207Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 0/200 | loss: 71.54 | learning rate: 0.000115\nRandom Tests\n******************************\nPred:  the is a father is of the of the of the EOS True:  at least we re still in one piece .\nPred:  i m a of the of the . EOS True:  i d like one more blanket .\nPred:  i don t know i don t know it . EOS True:  i didn t feel like going .\nPred:  we re not . EOS True:  we had lunch early .\nPred:  it is a of the of the . EOS True:  my father is in his room .\nPred:  the is the of the of the of the father the of the of the of the of the of the of the of the EOS True:  the british had military bases along new york s hudson river .\nPred:  it is a . EOS True:  that s rather amusing .\nPred:  you re not a of the ? EOS True:  does it hurt when you chew ?\nPred:  you re not to do you do ? EOS True:  didn t you know that oil floats on water ?\nPred:  she was a of the . EOS True:  she made the same mistake again .\n******************************\nEpoch 1/200 | loss: 46.19 | learning rate: 0.00023\nEpoch 2/200 | loss: 40.49 | learning rate: 0.000344\nEpoch 3/200 | loss: 37.55 | learning rate: 0.000459\nEpoch 4/200 | loss: 35.67 | learning rate: 0.000574\nEpoch 5/200 | loss: 35.34 | learning rate: 0.000689\nEpoch 6/200 | loss: 35.29 | learning rate: 0.000652\nEpoch 7/200 | loss: 34.45 | learning rate: 0.00061\nEpoch 8/200 | loss: 34.1 | learning rate: 0.000575\nEpoch 9/200 | loss: 33.4 | learning rate: 0.000545\nEpoch 10/200 | loss: 33.5 | learning rate: 0.00052\nEpoch 11/200 | loss: 33.04 | learning rate: 0.000498\nEpoch 12/200 | loss: 33.11 | learning rate: 0.000478\nEpoch 13/200 | loss: 32.46 | learning rate: 0.000461\nEpoch 14/200 | loss: 32.26 | learning rate: 0.000445\nEpoch 15/200 | loss: 32.3 | learning rate: 0.000431\nEpoch 16/200 | loss: 31.85 | learning rate: 0.000418\nEpoch 17/200 | loss: 32.27 | learning rate: 0.000406\nEpoch 18/200 | loss: 31.89 | learning rate: 0.000396\nEpoch 19/200 | loss: 32.15 | learning rate: 0.000386\nEpoch 20/200 | loss: 31.9 | learning rate: 0.000376\nEpoch 21/200 | loss: 31.57 | learning rate: 0.000368\nEpoch 22/200 | loss: 31.56 | learning rate: 0.00036\nEpoch 23/200 | loss: 31.26 | learning rate: 0.000352\nEpoch 24/200 | loss: 31.41 | learning rate: 0.000345\nEpoch 25/200 | loss: 31.3 | learning rate: 0.000338\nRandom Tests\n******************************\nPred:  to be a fool of the weather is well to time . EOS True:  studying how to communicate effectively is time well spent .\nPred:  we are among living in the library . EOS True:  we live near the large library .\nPred:  i m pretty busy . EOS True:  i m kind of busy right now tom .\nPred:  cars are more than the more can . EOS True:  oranges are sweeter than lemons .\nPred:  it s a big hear . EOS True:  it s a dead end relationship .\nPred:  i felt my phone . EOS True:  i felt my phone vibrate in my pocket .\nPred:  cars are more than people . EOS True:  oranges are sweeter than lemons .\nPred:  are you here ? EOS True:  are you there ?\nPred:  it s not not up to you . EOS True:  that s not for you to decide .\nPred:  i ll handle it . EOS True:  i ll handle this .\n******************************\nEpoch 26/200 | loss: 31.14 | learning rate: 0.000332\nEpoch 27/200 | loss: 30.58 | learning rate: 0.000326\nEpoch 28/200 | loss: 30.87 | learning rate: 0.00032\nEpoch 29/200 | loss: 30.74 | learning rate: 0.000315\nEpoch 30/200 | loss: 30.35 | learning rate: 0.00031\nEpoch 31/200 | loss: 30.63 | learning rate: 0.000305\nEpoch 32/200 | loss: 30.39 | learning rate: 0.0003\nEpoch 33/200 | loss: 30.05 | learning rate: 0.000296\nEpoch 34/200 | loss: 30.59 | learning rate: 0.000291\nEpoch 35/200 | loss: 29.97 | learning rate: 0.000287\nEpoch 36/200 | loss: 29.88 | learning rate: 0.000283\nEpoch 37/200 | loss: 30.04 | learning rate: 0.00028\nEpoch 38/200 | loss: 30.22 | learning rate: 0.000276\nEpoch 39/200 | loss: 29.84 | learning rate: 0.000273\nEpoch 40/200 | loss: 30.01 | learning rate: 0.000269\nEpoch 41/200 | loss: 30.03 | learning rate: 0.000266\nEpoch 42/200 | loss: 29.5 | learning rate: 0.000263\nEpoch 43/200 | loss: 29.79 | learning rate: 0.00026\nEpoch 44/200 | loss: 29.46 | learning rate: 0.000257\nEpoch 45/200 | loss: 29.38 | learning rate: 0.000254\nEpoch 46/200 | loss: 29.24 | learning rate: 0.000251\nEpoch 47/200 | loss: 29.54 | learning rate: 0.000249\nEpoch 48/200 | loss: 29.37 | learning rate: 0.000246\nEpoch 49/200 | loss: 28.93 | learning rate: 0.000244\nEpoch 50/200 | loss: 29.29 | learning rate: 0.000241\nRandom Tests\n******************************\nPred:  they have a problem . EOS True:  they have a problem .\nPred:  it seems that tom can t solve the problem . EOS True:  it seems that tom is unable to solve the problem .\nPred:  today is a great day . EOS True:  today is a very exciting day .\nPred:  the office is made of yours . EOS True:  the desk is made of wood .\nPred:  mary asked for a raise of his boss . EOS True:  mary requested a raise from her boss .\nPred:  i can t hear anything you say . EOS True:  i can t make heads or tails of what you say .\nPred:  it was only the you were the door . EOS True:  that was just the tip of the iceberg .\nPred:  nobody solved the problem . EOS True:  nobody has solved the problem .\nPred:  i don t have a lot of time . EOS True:  i don t have a lot of time .\nPred:  is what he said ? EOS True:  did he say that ?\n******************************\nEpoch 51/200 | loss: 28.72 | learning rate: 0.000239\nEpoch 52/200 | loss: 29.21 | learning rate: 0.000237\nEpoch 53/200 | loss: 29.06 | learning rate: 0.000235\nEpoch 54/200 | loss: 29.09 | learning rate: 0.000232\nEpoch 55/200 | loss: 28.92 | learning rate: 0.00023\nEpoch 56/200 | loss: 29.01 | learning rate: 0.000228\nEpoch 57/200 | loss: 29.33 | learning rate: 0.000226\nEpoch 58/200 | loss: 28.62 | learning rate: 0.000224\nEpoch 59/200 | loss: 29.03 | learning rate: 0.000223\nEpoch 60/200 | loss: 28.83 | learning rate: 0.000221\nEpoch 61/200 | loss: 28.69 | learning rate: 0.000219\nEpoch 62/200 | loss: 28.79 | learning rate: 0.000217\nEpoch 63/200 | loss: 28.91 | learning rate: 0.000216\nEpoch 64/200 | loss: 28.58 | learning rate: 0.000214\nEpoch 65/200 | loss: 28.57 | learning rate: 0.000212\nEpoch 66/200 | loss: 28.77 | learning rate: 0.000211\nEpoch 67/200 | loss: 28.47 | learning rate: 0.000209\nEpoch 68/200 | loss: 28.51 | learning rate: 0.000208\nEpoch 69/200 | loss: 28.36 | learning rate: 0.000206\nEpoch 70/200 | loss: 28.15 | learning rate: 0.000205\nEpoch 71/200 | loss: 28.15 | learning rate: 0.000203\nEpoch 72/200 | loss: 28.53 | learning rate: 0.000202\nEpoch 73/200 | loss: 28.37 | learning rate: 0.0002\nEpoch 74/200 | loss: 28.3 | learning rate: 0.000199\nEpoch 75/200 | loss: 28.17 | learning rate: 0.000198\nRandom Tests\n******************************\nPred:  this is more interesting . EOS True:  this is the most interesting .\nPred:  it s now or . EOS True:  it s now or never .\nPred:  don t you have any will you ? EOS True:  don t you have any will power ?\nPred:  she loves cake . EOS True:  she loves cake .\nPred:  you ll miss me very much . EOS True:  i m going to miss you a lot .\nPred:  i just decided to get home early . EOS True:  i just decided i d come home early .\nPred:  i think it s time for me to say that i really right . EOS True:  i think it s time for me to say what i really think .\nPred:  she wasn t able to get him to touch touch with the phone . EOS True:  she wasn t able to contact him by phone .\nPred:  the two countries is a high for a . EOS True:  the two countries are engaged in biological warfare .\nPred:  haven t you ever promised to lie ? EOS True:  didn t you promise never to tell a lie ?\n******************************\nEpoch 76/200 | loss: 28.17 | learning rate: 0.000196\nEpoch 77/200 | loss: 28.37 | learning rate: 0.000195\nEpoch 78/200 | loss: 27.97 | learning rate: 0.000194\nEpoch 79/200 | loss: 28.01 | learning rate: 0.000193\nEpoch 80/200 | loss: 27.87 | learning rate: 0.000192\nEpoch 81/200 | loss: 27.67 | learning rate: 0.00019\nEpoch 82/200 | loss: 27.93 | learning rate: 0.000189\nEpoch 83/200 | loss: 27.98 | learning rate: 0.000188\nEpoch 84/200 | loss: 28.16 | learning rate: 0.000187\nEpoch 85/200 | loss: 28.38 | learning rate: 0.000186\nEpoch 86/200 | loss: 27.93 | learning rate: 0.000185\nEpoch 87/200 | loss: 27.83 | learning rate: 0.000184\nEpoch 88/200 | loss: 27.79 | learning rate: 0.000183\nEpoch 89/200 | loss: 27.74 | learning rate: 0.000182\nEpoch 90/200 | loss: 27.67 | learning rate: 0.000181\nEpoch 91/200 | loss: 27.78 | learning rate: 0.00018\nEpoch 92/200 | loss: 27.8 | learning rate: 0.000179\nEpoch 93/200 | loss: 27.68 | learning rate: 0.000178\nEpoch 94/200 | loss: 27.59 | learning rate: 0.000177\nEpoch 95/200 | loss: 27.78 | learning rate: 0.000176\nEpoch 96/200 | loss: 27.04 | learning rate: 0.000175\nEpoch 97/200 | loss: 27.41 | learning rate: 0.000174\nEpoch 98/200 | loss: 27.86 | learning rate: 0.000173\nEpoch 99/200 | loss: 27.43 | learning rate: 0.000172\nEpoch 100/200 | loss: 27.59 | learning rate: 0.000172\nRandom Tests\n******************************\nPred:  he could have helped his sister paint the wall . EOS True:  he had his sister help him paint the wall of his room .\nPred:  i used to make a letter of love . EOS True:  i was writing her a love letter .\nPred:  the wind has set the leaves . EOS True:  the wind scattered the leaves about .\nPred:  the simple fact that i think is enough to make me happy . EOS True:  the mere thought of it is enough to make me happy .\nPred:  they got well . EOS True:  they handled it well .\nPred:  i m afraid we re not . EOS True:  i m afraid we don t have any left .\nPred:  i want to see your mother . EOS True:  i want to see your mother .\nPred:  i can t believe it happened to me . EOS True:  i can t believe this is really happening to me .\nPred:  we re moving . EOS True:  we re surviving .\nPred:  i didn t have the heart to go . EOS True:  i didn t feel like going .\n******************************\nEpoch 101/200 | loss: 27.52 | learning rate: 0.000171\nEpoch 102/200 | loss: 27.66 | learning rate: 0.00017\nEpoch 103/200 | loss: 27.34 | learning rate: 0.000169\nEpoch 104/200 | loss: 27.49 | learning rate: 0.000168\nEpoch 105/200 | loss: 27.13 | learning rate: 0.000167\nEpoch 106/200 | loss: 27.26 | learning rate: 0.000167\nEpoch 107/200 | loss: 27.62 | learning rate: 0.000166\nEpoch 108/200 | loss: 27.23 | learning rate: 0.000165\nEpoch 109/200 | loss: 27.31 | learning rate: 0.000164\nEpoch 110/200 | loss: 27.43 | learning rate: 0.000164\nEpoch 111/200 | loss: 27.46 | learning rate: 0.000163\nEpoch 112/200 | loss: 27.11 | learning rate: 0.000162\nEpoch 113/200 | loss: 27.12 | learning rate: 0.000161\nEpoch 114/200 | loss: 27.26 | learning rate: 0.000161\nEpoch 115/200 | loss: 27.09 | learning rate: 0.00016\nEpoch 116/200 | loss: 27.74 | learning rate: 0.000159\nEpoch 117/200 | loss: 27.01 | learning rate: 0.000159\nEpoch 118/200 | loss: 27.22 | learning rate: 0.000158\nEpoch 119/200 | loss: 27.27 | learning rate: 0.000157\nEpoch 120/200 | loss: 26.91 | learning rate: 0.000157\nEpoch 121/200 | loss: 26.93 | learning rate: 0.000156\nEpoch 122/200 | loss: 26.98 | learning rate: 0.000155\nEpoch 123/200 | loss: 26.74 | learning rate: 0.000155\nEpoch 124/200 | loss: 26.86 | learning rate: 0.000154\nEpoch 125/200 | loss: 27.09 | learning rate: 0.000154\nRandom Tests\n******************************\nPred:  i think life is what you re doing . EOS True:  i think life is what you make it .\nPred:  i just want you to know how much i like that . EOS True:  i just want you to know how much i appreciate this .\nPred:  we live near the library . EOS True:  we live near the large library .\nPred:  take the taste out . EOS True:  take out the trash .\nPred:   you re a good guitarist . i d like to think i am .  EOS True:   you re a good guitarist . i d like to think i am . \nPred:  i called you . EOS True:  i called you .\nPred:  you don t need to be in such a hurry . EOS True:  you don t need to be in such a hurry .\nPred:  the sight of a beauty is beautiful . EOS True:  the view is beautiful beyond words .\nPred:  i borrowed this book from her . EOS True:  i borrowed this book from him .\nPred:  you re a s going . EOS True:  you re temperamental .\n******************************\nEpoch 126/200 | loss: 26.51 | learning rate: 0.000153\nEpoch 127/200 | loss: 26.92 | learning rate: 0.000152\nEpoch 128/200 | loss: 26.66 | learning rate: 0.000152\nEpoch 129/200 | loss: 26.53 | learning rate: 0.000151\nEpoch 130/200 | loss: 26.82 | learning rate: 0.000151\nEpoch 131/200 | loss: 26.81 | learning rate: 0.00015\nEpoch 132/200 | loss: 26.73 | learning rate: 0.00015\nEpoch 133/200 | loss: 27.09 | learning rate: 0.000149\nEpoch 134/200 | loss: 26.88 | learning rate: 0.000148\nEpoch 135/200 | loss: 26.66 | learning rate: 0.000148\nEpoch 136/200 | loss: 26.81 | learning rate: 0.000147\nEpoch 137/200 | loss: 26.45 | learning rate: 0.000147\nEpoch 138/200 | loss: 26.66 | learning rate: 0.000146\nEpoch 139/200 | loss: 26.52 | learning rate: 0.000146\nEpoch 140/200 | loss: 26.65 | learning rate: 0.000145\nEpoch 141/200 | loss: 26.52 | learning rate: 0.000145\nEpoch 142/200 | loss: 26.31 | learning rate: 0.000144\nEpoch 143/200 | loss: 26.72 | learning rate: 0.000144\nEpoch 144/200 | loss: 26.05 | learning rate: 0.000143\nEpoch 145/200 | loss: 26.63 | learning rate: 0.000143\nEpoch 146/200 | loss: 26.29 | learning rate: 0.000142\nEpoch 147/200 | loss: 26.41 | learning rate: 0.000142\nEpoch 148/200 | loss: 26.46 | learning rate: 0.000141\nEpoch 149/200 | loss: 26.43 | learning rate: 0.000141\nEpoch 150/200 | loss: 26.32 | learning rate: 0.00014\nRandom Tests\n******************************\nPred:  is there anything i can get you ? EOS True:  is there something i can get for you ?\nPred:  where exactly did you go ? EOS True:  where exactly did you go ?\nPred:  that s kind of fun . EOS True:  that s rather amusing .\nPred:  i d like a cup of coffee . EOS True:  i would like a cup of coffee .\nPred:  would you like to see my new car ? EOS True:  would you like to see my new car ?\nPred:  try not to do not to do that . EOS True:  try not to worry .\nPred:  do you feel like a drink ? EOS True:  do you feel like having a drink ?\nPred:  i agree with what you ve written . EOS True:  i agree with what you ve written .\nPred:  who called ? EOS True:  who called you ?\nPred:  tom never did it again . EOS True:  tom never did it again .\n******************************\nEpoch 151/200 | loss: 26.62 | learning rate: 0.00014\nEpoch 152/200 | loss: 26.36 | learning rate: 0.000139\nEpoch 153/200 | loss: 26.51 | learning rate: 0.000139\nEpoch 154/200 | loss: 26.35 | learning rate: 0.000138\nEpoch 155/200 | loss: 26.52 | learning rate: 0.000138\nEpoch 156/200 | loss: 26.35 | learning rate: 0.000138\nEpoch 157/200 | loss: 26.86 | learning rate: 0.000137\nEpoch 158/200 | loss: 26.37 | learning rate: 0.000137\nEpoch 159/200 | loss: 26.29 | learning rate: 0.000136\nEpoch 160/200 | loss: 26.43 | learning rate: 0.000136\nEpoch 161/200 | loss: 26.23 | learning rate: 0.000135\nEpoch 162/200 | loss: 26.48 | learning rate: 0.000135\nEpoch 163/200 | loss: 26.33 | learning rate: 0.000135\nEpoch 164/200 | loss: 26.16 | learning rate: 0.000134\nEpoch 165/200 | loss: 26.27 | learning rate: 0.000134\nEpoch 166/200 | loss: 26.28 | learning rate: 0.000133\nEpoch 167/200 | loss: 26.54 | learning rate: 0.000133\nEpoch 168/200 | loss: 26.47 | learning rate: 0.000133\nEpoch 169/200 | loss: 26.31 | learning rate: 0.000132\nEpoch 170/200 | loss: 25.98 | learning rate: 0.000132\nEpoch 171/200 | loss: 26.16 | learning rate: 0.000131\nEpoch 172/200 | loss: 25.9 | learning rate: 0.000131\nEpoch 173/200 | loss: 26.38 | learning rate: 0.000131\nEpoch 174/200 | loss: 26.08 | learning rate: 0.00013\nEpoch 175/200 | loss: 26.19 | learning rate: 0.00013\nRandom Tests\n******************************\nPred:  i don t you get my face before i kept my face . EOS True:  i lather my face before shaving .\nPred:  it s nice today . EOS True:  the weather is good today .\nPred:  i saw the movie . EOS True:  i watched the movie .\nPred:  he works in full of the . EOS True:  he is working in aids research .\nPred:  she accompanied the piano to the piano . EOS True:  she accompanied him on the piano .\nPred:  my mother has a milk . EOS True:  my mother tasted the milk .\nPred:  how can this be possible ? EOS True:  how can it be ?\nPred:  don t ask me again . EOS True:  don t ask me that again .\nPred:  i just decided that i was going home early . EOS True:  i just decided i d come home early .\nPred:  she has very strong her daughter . EOS True:  she has a very strong personality .\n******************************\nEpoch 176/200 | loss: 26.31 | learning rate: 0.00013\nEpoch 177/200 | loss: 26.36 | learning rate: 0.000129\nEpoch 178/200 | loss: 26.15 | learning rate: 0.000129\nEpoch 179/200 | loss: 25.85 | learning rate: 0.000129\nEpoch 180/200 | loss: 25.97 | learning rate: 0.000128\nEpoch 181/200 | loss: 26.11 | learning rate: 0.000128\nEpoch 182/200 | loss: 26.05 | learning rate: 0.000127\nEpoch 183/200 | loss: 26.17 | learning rate: 0.000127\nEpoch 184/200 | loss: 26.06 | learning rate: 0.000127\nEpoch 185/200 | loss: 25.95 | learning rate: 0.000126\nEpoch 186/200 | loss: 26.23 | learning rate: 0.000126\nEpoch 187/200 | loss: 26.12 | learning rate: 0.000126\nEpoch 188/200 | loss: 26.03 | learning rate: 0.000125\nEpoch 189/200 | loss: 25.82 | learning rate: 0.000125\nEpoch 190/200 | loss: 25.95 | learning rate: 0.000125\nEpoch 191/200 | loss: 25.77 | learning rate: 0.000124\nEpoch 192/200 | loss: 25.88 | learning rate: 0.000124\nEpoch 193/200 | loss: 25.98 | learning rate: 0.000124\nEpoch 194/200 | loss: 25.88 | learning rate: 0.000123\nEpoch 195/200 | loss: 25.66 | learning rate: 0.000123\nEpoch 196/200 | loss: 25.69 | learning rate: 0.000123\nEpoch 197/200 | loss: 25.89 | learning rate: 0.000123\nEpoch 198/200 | loss: 25.55 | learning rate: 0.000122\nEpoch 199/200 | loss: 26.04 | learning rate: 0.000122\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#more random tests \nrandom_model_testing(20,transformer1)","metadata":{"execution":{"iopub.status.busy":"2026-01-11T18:26:27.364480Z","iopub.execute_input":"2026-01-11T18:26:27.364763Z","iopub.status.idle":"2026-01-11T18:26:29.824124Z","shell.execute_reply.started":"2026-01-11T18:26:27.364738Z","shell.execute_reply":"2026-01-11T18:26:29.823203Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Random Tests\n******************************\nPred:  you can do it too . EOS True:  you can do it too .\nPred:  you re wrong . EOS True:  you are wrong .\nPred:  she has much better to cook recently . EOS True:  she has improved her skill in cooking recently .\nPred:  don t make any prisoner . EOS True:  don t take any prisoners .\nPred:  don t touch my bicycle . EOS True:  keep your hands off my bicycle .\nPred:  i live here alone . EOS True:  i live here alone .\nPred:  don t be so impatient . EOS True:  don t be so impatient .\nPred:  why am i so tired ? EOS True:  why am i so tired ?\nPred:  i know i gave the right thing this time . EOS True:  i know i got it right this time .\nPred:  this is not a no town . EOS True:  it s non refundable .\nPred:  he is the tallest of the two . EOS True:  he is the taller of the two .\nPred:  i don t know what s fear . EOS True:  i don t know what fear is .\nPred:  i m being on your age when you knew how to drive . EOS True:  i ll treat you like an adult when you start acting like one .\nPred:  who is proud of it ? EOS True:  who can you trust ?\nPred:  my girlfriend was crying . EOS True:  my girlfriend was crying .\nPred:  isn t it about the money ? EOS True:  it s not about the money is it ?\nPred:  anybody can t make mistakes . EOS True:  anyone can make mistakes .\nPred:  she wants a serious about the serious . EOS True:  she wants a serious relationship .\nPred:  he is nervous in america . EOS True:  he is anxious to go to america .\nPred:  i saw you outside . EOS True:  i saw you outside .\n******************************\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Conclusion\n\nThe model does a decent job after approximately an hour of training on GPU. Of course for better results more training and more data would do the trick.\n\nThis was the first time I implemented a high quality deep-learning paper. I learned that this model is \"sensitive\", meaning that small errors in the implementation will make the model very *unreasonable*. ü§™ It took some patience, but, in order to understand the topic there is no better way. Hopefully, this is helpful to somebody out there... üòä  ","metadata":{}}]}